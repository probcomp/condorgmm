{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose proposals via stratified (grid) sampling\n",
    "\n",
    "We add an option in the inference to generate pose proposals via sampling in a uniform pose grid, rather than the default of Gaussian-VMF pose proposals.  \n",
    "\n",
    "This notebook visualizes such example grids, and demonstrates a run of the end-to-end pipeline with `use_gt_pose=False`, \n",
    "across the first 2 scenes for each object. \n",
    "We also will aggregate metrics to confirm that pose gridding appears to work at the integrated inference level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.spatial.transform import Rotation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import condorgmm \n",
    "from condorgmm import Pose, Mesh\n",
    "from condorgmm.utils.pose_gridding import make_pose_grid_enumeration_simple_jit, rr_log_pose_grid, rr_log_cloud_and_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: grid visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate pose proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = jax.random.PRNGKey(1222)\n",
    "previous_pose = Pose(jnp.array([0.0, 0.0, 0.0]), Pose.identity_quaternion)\n",
    "n_translation_half_dim = 2\n",
    "n_rotation_half_dim = 2\n",
    "n_poses = (2 * n_translation_half_dim + 1)**3 * (2 * n_rotation_half_dim + 1)**3\n",
    "print(f\"Generating {n_poses} poses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian VMF proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2f_schedule = [\n",
    "    (0.04, 1500.0),\n",
    "    (0.02, 2000.0),\n",
    "    (0.01, 2000.0),\n",
    "    (0.005, 3000.0),\n",
    "] ## in end_to_end.py#L127\n",
    "std, concentration = c2f_schedule[0]\n",
    "print(f\"std: {std}, concentration: {concentration}\")\n",
    "\n",
    "vmf_poses = Pose.sample_gaussian_vmf_pose_vmap(\n",
    "    jax.random.split(k1, n_poses), previous_pose, std, concentration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get approximate \"parity\" between the range of pose deltas\n",
    "# generated by the vmf and the uniform grid, we initialize the\n",
    "# pose grid based on results of the vmf.\n",
    "# (NOTE tha the vmf is less interpretable in this way)\n",
    "\n",
    "translation_delta = jnp.max(vmf_poses.position.max(axis=0))\n",
    "rotation_euler_delta = Rotation.from_quat(vmf_poses.quaternion).as_euler(\"ZYX\").max()\n",
    "\n",
    "### Samples from gaussian vmf\n",
    "grid_poses= make_pose_grid_enumeration_simple_jit(\n",
    "    pose_center=previous_pose,\n",
    "    half_dtr=translation_delta,\n",
    "    half_ntr=n_translation_half_dim,\n",
    "    half_dangle=rotation_euler_delta,\n",
    "    half_nrot=n_rotation_half_dim,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize pose proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YCB_OBJ_ID = 13   # beloved mug\n",
    "ycb_mesh_dir = os.path.join(condorgmm.get_assets_path(), \"bop/ycbv\")\n",
    "mesh = Mesh.from_obj_file(\n",
    "    os.path.join(ycb_mesh_dir, f'models/obj_{f\"{YCB_OBJ_ID + 1}\".rjust(6, \"0\")}.ply')\n",
    ").scale(0.001)      # note that this processing is the same as in `rr_log_pose_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for a more immediately visually comparable Rerun log, \n",
    "# we sort the Pose in the posegrid by the translation distance\n",
    "\n",
    "vmf_sort = jnp.argsort(jnp.linalg.norm(vmf_poses.position - previous_pose.position, axis=1))\n",
    "grid_sort = jnp.argsort(jnp.linalg.norm(grid_poses.position - previous_pose.position, axis=1))\n",
    "grid_poses = grid_poses[grid_sort]\n",
    "vmf_poses = vmf_poses[grid_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_VIZ = True \n",
    "\n",
    "if DEBUG_VIZ:\n",
    "    # rr_log_pose_grid(vmf_poses, rerun_session_name=\"vmf_vs_grid\", ycb_obj_id=YCB_OBJ_ID, channel_name=\"vmf\")\n",
    "    rr_log_pose_grid(grid_poses, rerun_session_name=\"vmf_vs_grid\", ycb_obj_id=YCB_OBJ_ID, channel_name=\"grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulate inference with grid vs vmf based pose proposal\n",
    "We will decompose inference.py's `c2f_inference_step` to compare results on a single step of inference with the alternate pose gridding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup \n",
    "from condorgmm import load_scene\n",
    "from condorgmm.model import viz_trace\n",
    "from condorgmm.end_to_end import init_metrics_dict, get_initial_state_for_object, initialize_inference, update_hyperparams_for_subsequent_frames, extend_metrics\n",
    "from condorgmm.end_to_end import run_inference_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inference settings\n",
    "use_gt_pose = False  \n",
    "use_grid = True\n",
    "use_vmf = not use_grid\n",
    "\n",
    "## ycb scene/object to test\n",
    "YCB_SCENE = 1\n",
    "OBJECT_INDEX = 0   # obj index in scene\n",
    "FRAME_RATE = 50\n",
    "ycb_dir = condorgmm.get_root_path() / \"assets/bop/ycbv/train_real\"\n",
    "live_rerun = True\n",
    "save_rerun = True\n",
    "RERUN_SESSION_NAME = f\"ycbv_grid_debug_grid_{use_grid}\"\n",
    "from condorgmm.config.default import configuration as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load ycb\n",
    "all_data, meshes, intrinsics = load_scene(ycb_dir, YCB_SCENE, FRAME_RATE)\n",
    "initial_object_poses = all_data[0][\"object_poses\"]\n",
    "all_scores = init_metrics_dict()\n",
    "gt_pose = lambda T: all_data[T][\"camera_pose\"].inv() @ all_data[T][\"object_poses\"][OBJECT_INDEX]   # from end_to_end.py#L272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below code is taken from end_to_end.py/run_tracking\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "if live_rerun:\n",
    "    condorgmm.rr_init(RERUN_SESSION_NAME)\n",
    "    \n",
    "T = 0\n",
    "\n",
    "initial_object_poses = (\n",
    "    all_data[T][\"camera_pose\"].inv() @ all_data[T][\"object_poses\"]\n",
    ")\n",
    "\n",
    "initial_state = get_initial_state_for_object(\n",
    "    meshes, OBJECT_INDEX, initial_object_poses\n",
    ")\n",
    "\n",
    "trace = initialize_inference(\n",
    "    initial_state,\n",
    "    all_data,\n",
    "    config.model_hyperparams_first_frame,\n",
    "    meshes[OBJECT_INDEX].vertices,\n",
    "    intrinsics,\n",
    ")\n",
    "if live_rerun or save_rerun:\n",
    "    # Log to rerun as \"frame -1\"\n",
    "    viz_trace(trace, -1, meshes[OBJECT_INDEX].vertices, gt_pose(0))\n",
    "\n",
    "trace, _, metadata = (\n",
    "    condorgmm.inference.update_all_variables_given_pose(\n",
    "        key,\n",
    "        trace,\n",
    "        trace.get_choices()[\"pose\"],\n",
    "        config.point_attribute_proposal,\n",
    "    )\n",
    ")\n",
    "\n",
    "if live_rerun or save_rerun:\n",
    "    viz_trace(trace, 0, meshes[OBJECT_INDEX].vertices, gt_pose(0))\n",
    "                    \n",
    "# Run inference for the rest of the frames\n",
    "trace = update_hyperparams_for_subsequent_frames(\n",
    "    trace, config.model_hyperparams_subsequent_frames\n",
    ")\n",
    "\n",
    "inferred_poses = [trace.get_choices()[\"pose\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from inference.py\n",
    "def run_inference_step(\n",
    "    trace,\n",
    "    gt_pose,\n",
    "    use_gt_pose,\n",
    "    point_attribute_proposal,\n",
    "    observed_img,\n",
    "    key=jax.random.PRNGKey(0),\n",
    "    do_advance_time=True,\n",
    "    use_grid=False\n",
    "):\n",
    "    if do_advance_time:\n",
    "        trace = condorgmm.inference.advance_time(key, trace, observed_img)\n",
    "        \n",
    "    if not use_grid:\n",
    "        ## VMF pose proposals\n",
    "        c2f_schedule = [\n",
    "            (0.04, 1500.0),\n",
    "            (0.02, 2000.0),\n",
    "            (0.01, 2000.0),\n",
    "            (0.005, 3000.0),\n",
    "        ]\n",
    "        n_poses = 2000  \n",
    "        for v, c in c2f_schedule:\n",
    "            key = jax.random.split(key)[-1]\n",
    "            trace, metadata = condorgmm.inference.c2f_inference_gaussian_vmf(\n",
    "                key,\n",
    "                trace,\n",
    "                v, c,\n",
    "                n_poses,\n",
    "                point_attribute_proposal,\n",
    "                use_gt_pose=use_gt_pose,\n",
    "                gt_pose=gt_pose,\n",
    "                get_metadata=True    # for visualization\n",
    "            ) \n",
    "        \n",
    "    else:\n",
    "        ## gridding pose proposals\n",
    "        n_translation_half_dim = 2\n",
    "        n_rotation_half_dim = 1\n",
    "        c2f_schedule = [\n",
    "            # half_dtr, half_dangle\n",
    "            (0.015, jnp.pi/10),   \n",
    "            (0.0075 , jnp.pi/12),\n",
    "            (0.0025, jnp.pi/15),\n",
    "            (0.001, jnp.pi/20)\n",
    "        ]\n",
    "\n",
    "        for half_dtr, half_dangle in c2f_schedule:\n",
    "            key = jax.random.split(key)[-1]\n",
    "            trace, metadata = condorgmm.inference.c2f_inference_grid(\n",
    "                key,\n",
    "                trace,\n",
    "                half_dtr, half_dangle,\n",
    "                n_translation_half_dim, n_rotation_half_dim,\n",
    "                point_attribute_proposal,\n",
    "                use_gt_pose=use_gt_pose,\n",
    "                gt_pose=gt_pose,\n",
    "                get_metadata=True\n",
    "            )\n",
    "        \n",
    "    return trace, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxT = 10\n",
    "\n",
    "for T in tqdm(range(1, maxT)):\n",
    "    key, sk = jax.random.split(key)\n",
    "    trace, metadata = run_inference_step(\n",
    "                trace,\n",
    "                gt_pose(0),\n",
    "                use_gt_pose,\n",
    "                config.point_attribute_proposal,\n",
    "                all_data[0][\"rgbd\"],\n",
    "                sk,\n",
    "                do_advance_time=False,\n",
    "                use_grid=use_grid,   \n",
    "            )\n",
    "    inferred_poses.append(trace.get_choices()[\"pose\"])\n",
    "    print(trace.get_choices()['pose'].position)\n",
    "    \n",
    "    # if live_rerun or save_rerun:\n",
    "    #     viz_trace(trace, T, meshes[OBJECT_INDEX].vertices, gt_pose(T))\n",
    "    #     for sample_idx_to_viz in range(0,len(metadata['poses']), 100):\n",
    "    #         rr_log_cloud_and_pose(metadata['poses'][sample_idx_to_viz], \n",
    "    #         vertices=meshes[OBJECT_INDEX].vertices, t=T, \n",
    "    #         channel=\"grid_proposals\" if use_grid else \"vmf_proposals\")\n",
    "                        \n",
    "    extend_metrics(\n",
    "        all_scores,\n",
    "        trace,\n",
    "        \"test_object\",\n",
    "        gt_pose(T),\n",
    "        meshes[OBJECT_INDEX],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condorgmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
